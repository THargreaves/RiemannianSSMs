"""
Pretty damn fast now. Few slow-downs due to allocations which can easily be made in-place.

Roughly 25% of time is spent allocating. Some of these are easy to fix, some such as the
ones generated by addition are a bit harder.

Could consider static arrays of static arrays but worry about when K is large. In-place
operations might be the cleanest method.

This is at K = 20 though and as dimension grows this becomes more negligible.
"""

using Distributions
using JLD2
using LinearAlgebra
using MCMCDiagnosticTools
using Plots
using ProgressMeter
using Random
using StaticArrays

using RiemannianSSMs

SEED = 4
rng = MersenneTwister(SEED)

K = 100
δt = 1.0

μ0 = @SVector [0.0, 0.0, 0.0, 0.0]
Σ0 = Diagonal(@SVector([0.5, 0.5, 0.1, 0.1]))

prior = MvNormal(μ0, Σ0)

α = 0.01
β = 0.1
γ = 0.005
σ_p = 0.01
σ_v = 0.5

dyn = VariableRestoringForceDynamics(α, β, γ, σ_p, σ_v, δt)

# a1, b1 = -5.0, 0.0
# a2, b2 = 5.0, 0.0
# Try moving these away from trajectory to resolve numerical stability issues
a1, b1 = -1.0, -3.0
a2, b2 = 5.0, -1.5
σ1 = 0.5
σ2 = 0.5

obs = II(a1, b1, a2, b2, σ1, σ2)

struct SSM{PT,DY,OM}
    prior::PT
    dyn::DY
    sensor::OM
end
ssm = SSM(prior, dyn, obs);

function simulate(rng::AbstractRNG, ssm, K::Int)
    zs = Vector{SVector{4,Float64}}(undef, K)
    ys = Vector{SVector{2,Float64}}(undef, K)

    for k in 1:K
        if k == 1
            z = SVector{4,Float64}(rand(rng, ssm.prior))
        else
            z = f(ssm.dyn, zs[k - 1]) + rand(rng, MvNormal(zeros(4), calc_Q(ssm.dyn)))
        end
        zs[k] = z

        y = h(ssm.sensor, z) + rand(rng, MvNormal(zeros(2), calc_R(ssm.sensor)))
        ys[k] = y
    end

    return zs, ys
end
zs_true, ys = simulate(rng, ssm, K);

zs_true = BlockVector{Float64,4}(zs_true)

N_samples = 1000
N_burnin = 200
REPS = 5

function sample!(
    zs_samples,
    rng::AbstractRNG,
    ssm::SSM,
    z_init::BlockVector{Float64,4},
    ys::Vector{SVector{2,Float64}},
    N_samples::Int,
    N_burnin::Int,
    n_steps::Int,
    lf_params::LeapfrogParams{Float64};
    progress=true,
)
    zs_curr = copy(z_init)
    n_accept = 0
    n_divergent = 0
    K = length(ys)

    prog = Progress(N_samples; enabled=progress)
    for i in 1:N_samples
        G = calc_G(zs_curr, ssm)
        G_chol = cholesky(G)
        U = BlockVector{Float64,4}([@SVector randn(rng, 4) for k in 1:K])
        ps_curr = G_chol.U.data' * U
        H_curr = calc_hamiltonian(zs_curr, ps_curr, ys, ssm)
        zs_new = copy(zs_curr)
        ps_new = copy(ps_curr)

        early_reject = false
        for _ in 1:n_steps
            zs_new, ps_new, diverged = glf_step(zs_new, ps_new, ys, ssm, lf_params)
            if diverged
                early_reject = true
                break
            end
        end

        if early_reject
            zs_samples[i] = copy(zs_curr)
            n_divergent += 1
            next!(prog)
            continue
        end

        # Accept or reject
        H_new = calc_hamiltonian(zs_new, ps_new, ys, ssm)

        if log(rand(rng)) < H_curr - H_new
            zs_curr = zs_new
            n_accept += 1
        end

        zs_samples[i] = copy(zs_curr)
        next!(prog)
    end

    return zs_samples
end

z_init = zs_true
zs_samples = Vector{BlockVector{Float64,4}}(undef, N_samples)

# Define grid for step sizes and number of steps
step_sizes = 10 .^ range(-1.5, 0; length=8)
n_steps_list = round.(Int, 10 .^ range(0, 1.5; length=8))

# Weight the progress bar by number of steps
prog = Progress(length(step_sizes) * sum(n_steps_list) * REPS; desc="Grid Search")
flattened_idxs = collect(Iterators.product(1:length(step_sizes), 1:length(n_steps_list)))
grid_esses = Matrix{Vector{Float64}}(undef, length(step_sizes), length(n_steps_list))
Threads.@threads for (i, j) in flattened_idxs
    step_size = step_sizes[i]
    n_steps = n_steps_list[j]
    lf_params = LeapfrogParams{Float64}(step_size, 1e-6, 1000)

    reps_zs_samples = [Vector{BlockVector{Float64,4}}(undef, N_samples) for _ in 1:REPS]

    try
        for rep in 1:REPS
            sample!(
                reps_zs_samples[rep],
                rng,
                ssm,
                z_init,
                ys,
                N_samples,
                N_burnin,
                n_steps,
                lf_params;
                progress=false,
            )

            # Discard burn-in samples
            reps_zs_samples[rep] = reps_zs_samples[rep][(N_burnin + 1):end]

            for _ in 1:n_steps
                next!(prog)
            end
        end
    catch e
        if e isa LinearAlgebra.PosDefException
            grid_esses[i, j] = fill(NaN, 4 * K)
            for _ in 1:n_steps
                next!(prog)
            end
            continue
        else
            rethrow(e)
        end
    end

    # Shape into (draws, [chains[, parameters...]])
    combined_samples = Array{Float64,3}(undef, N_samples - N_burnin, REPS, 4 * K)
    for rep in 1:REPS
        for i in 1:(N_samples - N_burnin)
            for k in 1:K
                for d in 1:4
                    combined_samples[i, rep, (k - 1) * 4 + d] = reps_zs_samples[rep][i].blocks[k][d]
                end
            end
        end
    end

    grid_esses[i, j] = ess(combined_samples)
end

# Write results to file
@save "ess_grid_search_results.jld2" grid_esses step_sizes n_steps_list

# Read results from file
# @load "research/ess_grid_search_results.jld2" results step_sizes n_steps_list

# Plot minimum, median, and mean ESS across parameters for each (step_size, n_steps)
metrics = [
    "Minimum" => (x -> minimum(x)), "Median" => (x -> median(x)), "Mean" => (x -> mean(x))
]
metric_values = Dict{String,Float64}()
hs = []
for (metric_name, metric_func) in metrics
    metric_esses = Matrix{Float64}(undef, length(step_sizes), length(n_steps_list))
    for (i, step_size) in enumerate(step_sizes)
        for (j, n_steps) in enumerate(n_steps_list)
            ess_values = grid_esses[i, j] / n_steps / (N_samples - N_burnin)
            metric_esses[i, j] = metric_func(ess_values)
        end
    end
    metric_values[metric_name] = maximum(metric_esses[.!isnan.(metric_esses)])
    push!(
        hs,
        heatmap(
            n_steps_list,
            step_sizes,
            metric_esses;
            xlabel="Number of Leapfrog Steps",
            ylabel="Step Size",
            xscale=:log10,
            yscale=:log10,
            title="$metric_name Relative ESS per Step",
            colorbar_title="ESS",
        ),
    )
end
display(plot(hs...; layout=(length(hs), 1), size=(700, 1800)))

# Print best values for each metric
for (metric_name, value) in metric_values
    println("Best $metric_name ESS per step: $value")
end

savefig("ess_grid_search.png")
