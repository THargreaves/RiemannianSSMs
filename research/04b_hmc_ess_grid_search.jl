"""
Pretty damn fast now. Few slow-downs due to allocations which can easily be made in-place.

Roughly 25% of time is spent allocating. Some of these are easy to fix, some such as the
ones generated by addition are a bit harder.

Could consider static arrays of static arrays but worry about when K is large. In-place
operations might be the cleanest method.

This is at K = 20 though and as dimension grows this becomes more negligible.
"""

using Distributions
using JLD2
using LinearAlgebra
using MCMCDiagnosticTools
using Plots
using ProgressMeter
using Random
using StaticArrays

using RiemannianSSMs

SEED = 4
rng = MersenneTwister(SEED)

K = 100
δt = 1.0

μ0 = @SVector [0.0, 0.0, 0.0, 0.0]
Σ0 = Diagonal(@SVector([0.5, 0.5, 0.1, 0.1]))

prior = MvNormal(μ0, Σ0)

α = 0.01
β = 0.1
γ = 0.005
σ_p = 0.1
σ_v = 0.5

dyn = VariableRestoringForceDynamics(α, β, γ, σ_p, σ_v, δt)

# a1, b1 = -5.0, 0.0
# a2, b2 = 5.0, 0.0
# Try moving these away from trajectory to resolve numerical stability issues
a1, b1 = -1.0, -3.0
a2, b2 = 5.0, -1.5
σ1 = 0.5
σ2 = 0.5

obs = TwoLandmarkMeasurementModel(a1, b1, a2, b2, σ1, σ2)

struct SSM{PT,DY,OM}
    prior::PT
    dyn::DY
    sensor::OM
end
ssm = SSM(prior, dyn, obs);

function simulate(rng::AbstractRNG, ssm, K::Int)
    zs = Vector{SVector{4,Float64}}(undef, K)
    ys = Vector{SVector{2,Float64}}(undef, K)

    for k in 1:K
        if k == 1
            z = SVector{4,Float64}(rand(rng, ssm.prior))
        else
            z = f(ssm.dyn, zs[k - 1]) + rand(rng, MvNormal(zeros(4), calc_Q(ssm.dyn)))
        end
        zs[k] = z

        y = h(ssm.sensor, z) + rand(rng, MvNormal(zeros(2), calc_R(ssm.sensor)))
        ys[k] = y
    end

    return zs, ys
end
zs_true, ys = simulate(rng, ssm, K);

zs_true = BlockVector{Float64,4}(zs_true)

N_samples = 10000
N_burnin = 2000
REPS = 50

# Only need to search over number of steps since we can tune step size
n_steps_list = round.(Int, 10 .^ range(0, 3.0; length=20))
unique!(n_steps_list)

using AdvancedHMC, AbstractMCMC
using LogDensityProblems, LogDensityProblemsAD, ADTypes
using ForwardDiff
using DistributionsAD

struct LogTargetDensity{M,V}
    dim::Int
    ys::V
    ssm::M
end
function LogDensityProblems.logdensity(p::LogTargetDensity, θ)
    θ = BlockVector{Float64,4}(reinterpret(SVector{4,Float64}, θ))
    return RiemannianSSMs.calc_ll(θ, p.ys, p.ssm)
end
function LogDensityProblems.logdensity_and_gradient(p::LogTargetDensity, θ)
    θ = BlockVector{Float64,4}(reinterpret(SVector{4,Float64}, θ))
    ll = RiemannianSSMs.calc_ll(θ, p.ys, p.ssm)
    grad = RiemannianSSMs.calc_ll_grad(θ, p.ys, p.ssm)
    return ll, reinterpret(Float64, grad.blocks)
end
LogDensityProblems.dimension(p::LogTargetDensity) = p.dim
function LogDensityProblems.capabilities(::Type{<:LogTargetDensity})
    return LogDensityProblems.LogDensityOrder{1}()
end

ℓπ = LogTargetDensity(4 * K, ys, ssm)
model = AdvancedHMC.LogDensityModel(ℓπ)

initial_θ = reduce(vcat, zs_true)

# Sample once to profile
hmc = HMC(0.1, 10)
@profview AbstractMCMC.sample(
    model, hmc, 10000; n_adapts=5000, initial_params=initial_θ, verbose=false, progress=false
)

# Weight the progress bar by number of steps
prog = Progress(sum(n_steps_list) * REPS; desc="Grid Search")
grid_esses = Vector{Vector{Float64}}(undef, length(n_steps_list))
Threads.@threads for i in 1:length(n_steps_list)
    n_leapfrog = n_steps_list[i]
    metric = DiagEuclideanMetric(4K)
    hamiltonian = Hamiltonian(metric, ℓπ)
    initial_ϵ = find_good_stepsize(hamiltonian, initial_θ)
    integrator = Leapfrog(initial_ϵ)
    kernel = HMCKernel(Trajectory{EndPointTS}(integrator, FixedNSteps(n_leapfrog)))
    adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))
    hmc = HMCSampler(kernel, metric, adaptor)

    reps_samples = [Vector{Vector{Float64}}(undef, N_samples) for _ in 1:REPS]

    for rep in 1:REPS
        samples, stats = AbstractMCMC.sample(
            hamiltonian,
            kernel,
            initial_θ,
            N_samples,
            adaptor,
            N_burnin;
            verbose=false,
            progress=false,
        )
        reps_samples[rep] = samples[(N_burnin + 1):end]

        for _ in 1:n_leapfrog
            next!(prog)
        end
    end

    combined_samples = Array{Float64,3}(undef, N_samples - N_burnin, REPS, 4 * K)
    for rep in 1:REPS
        for i in 1:(N_samples - N_burnin)
            combined_samples[i, rep, :] = reps_samples[rep][i]
        end
    end

    grid_esses[i] = ess(combined_samples)
end

# @save "hmc_ess_grid_search_results.jld2" grid_esses n_steps_list

@load "research/hmc_ess_grid_search_results.jld2" grid_esses n_steps_list

# Plot minimum, median, and mean ESS per step across parameters for each n_steps
metrics = [
    "Minimum" => (x -> minimum(x)), "Median" => (x -> median(x)), "Mean" => (x -> mean(x))
]
metric_values = Dict{String,Float64}()
hs = []
for (metric_name, metric_func) in metrics
    metric_esses = Vector{Float64}(undef, length(n_steps_list))
    for (i, n_steps) in enumerate(n_steps_list)
        ess_values = grid_esses[i] / n_steps / (N_samples - N_burnin)
        metric_esses[i] =  metric_func(ess_values)
    end
    metric_values[metric_name] = maximum(metric_esses[.!isnan.(metric_esses)])
    push!(
        hs,
        plot(
            n_steps_list,
            metric_esses;
            xlabel="Number of Leapfrog Steps",
            ylabel="ESS per Step",
            xscale=:log10,
            # yscale=:log10,
            title="$metric_name ESS per Step",
            legend=false,
        ),
    )
end
display(plot(hs...; layout=(length(metrics), 1), size=(700, 1800), left_margin=75Plots.px))

# Print best values for each metric
for (metric_name, value) in metric_values
    println("Best $metric_name ESS per step: $value")
end

savefig("hmc_ess_grid_search.png")
